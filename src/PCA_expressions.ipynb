{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA on multiple expressions meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "- READ OBJs systematically\n",
    "    - Make them functions to be called later\n",
    "- RUN PCA (sklearn PCA)\n",
    "    - Make them functions to be called later\n",
    "    - __(+30Oct) Take into account only vertices which represebts the front face region which effectively used for facial expressions__\n",
    "- Save in .pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements/test-environment\n",
    "- Manjaro Linux\n",
    "- python 3.12.0\n",
    "- Anaconda 22.9.0\n",
    "    - packages-list\n",
    "        - see requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read OBJs\n",
    "- place downloaded tracked meshes of multiple expressions /dataset/multiface/tracked_mesh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# set path to dataset\n",
    "path_to_dataset = os.path.join(os.getcwd(), '../dataset/multiface/tracked_mesh/')\n",
    "save_path = \"../dataset/multiface/tracked_mesh\"\n",
    "\n",
    "ID = 6795937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of expressions\n",
    "list_exps_name = []\n",
    "map_exps2id = {}\n",
    "counter = 0\n",
    "for i, name in enumerate(os.listdir(path_to_dataset)):\n",
    "    f = os.path.join(path_to_dataset, name)\n",
    "    if os.path.isdir(f) and name.startswith('E0'):\n",
    "        counter = counter + 1\n",
    "        list_exps_name.append(name)\n",
    "\n",
    "\n",
    "list_exps_name.sort()\n",
    "\n",
    "for i, exp_name in enumerate(list_exps_name):\n",
    "    print(f'{i}, {exp_name}')\n",
    "    map_exps2id.update({exp_name: i})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read OBJ in ./dataset/multiface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of expressions\n",
    "print(list_exps_name)\n",
    "print(map_exps2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File handler for .obj in ./utils/FolderHandler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dataset_handler import Filehandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler = Filehandler(path_to_dataset=path_to_dataset)\n",
    "print(file_handler.get_path_to_dataset())\n",
    "file_handler.iter_dir()\n",
    "print(\"Expressions: Number of tracked mesh\")\n",
    "for key in file_handler.dict_objs.keys():\n",
    "    print(f'{list_exps_name[key]}: {len(file_handler.dict_objs[key])}')\n",
    "    print(file_handler.dict_objs[key]) \n",
    "# print(file_handler.dict_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.OBJ_helper import OBJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test .obj loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../dataset/multiface/tracked_mesh/E003_Neutral_Eyes_Closed/000783.obj\"\n",
    "test_obj = OBJ(test_path, swapyz=False)\n",
    "print(f'Number of vertices: {len(test_obj.vertices)}')\n",
    "print(test_obj.vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load all .obj in the certain directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_expOBJs = {}\n",
    "dict_expVerts = {}\n",
    "\n",
    "# selecgt number of samples for a expression \n",
    "num_samples_perExp = 20\n",
    "\n",
    "for expID, key in enumerate(file_handler.dict_objs.keys()):\n",
    "    list_OBJs = []\n",
    "    list_Verts = []\n",
    "\n",
    "    # since there are many sequences for a expression, we assume that second half of tracked mesh in a sequence captured the specific expressions\n",
    "    # We only use second half of sequence for a expression.\n",
    "    # This is also for resonable memory usage as well. if you run over all, you will consume more than 30GB memory to store all of objects\n",
    "\n",
    "    # half_id = int(len(file_handler.dict_objs[key])/2)\n",
    "    # end_id = int(len(file_handler.dict_objs[key]))\n",
    "\n",
    "    for i, obj in enumerate(file_handler.dict_objs[key][0:num_samples_perExp]):\n",
    "        path_to_obj = os.path.join(file_handler.list_expPathFiles[expID], obj)\n",
    "        # print(path_to_obj)\n",
    "        obj = OBJ(path_to_obj, swapyz=False)\n",
    "        list_OBJs.append(obj)\n",
    "        list_Verts.append(obj.vertices)\n",
    "    dict_expOBJs.update({expID: list_OBJs})\n",
    "    dict_expVerts.update({expID: list_Verts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the number of objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in dict_expVerts.keys():\n",
    "#     vertices = dict_expVerts[key]\n",
    "#     print(len(vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test conversion from list to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp0_vertices = dict_expVerts[0]\n",
    "print(f\"Expression: {list_exps_name[0]}\")\n",
    "print(f\"The number of meshes: {len(exp0_vertices)}\")\n",
    "print(f\"The number of vertices for each mesh: {len(exp0_vertices[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "num_vertices = 7306\n",
    "len_col = num_vertices*3\n",
    "print(f\"The number of vertices for each meesh: {num_vertices}\")\n",
    "print(f\"The number of columns of matrix X: {len_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test concatenation and check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_np_array = np.array(exp0_vertices)\n",
    "print(_np_array.shape)\n",
    "print(_np_array[0][0][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_array = np.array(exp0_vertices).reshape((_np_array.shape[0], len_col))\n",
    "print(_array.shape)\n",
    "print(_array[0][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation of vertex positions for all tracked meshes of all expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_xs = []\n",
    "num_sum_samples = 0\n",
    "for key in dict_expVerts.keys():\n",
    "    vertices = dict_expVerts[key]\n",
    "    _num_samples = len(vertices)\n",
    "    print(_num_samples)\n",
    "    num_sum_samples = num_sum_samples + _num_samples\n",
    "    _array = np.array(vertices).reshape((_num_samples, len_col))\n",
    "    _list_xs.append(_array)\n",
    "\n",
    "# print(f\"Len of _list_xs: {len(_list_xs)}\")\n",
    "# print(f\"The number of samples: {num_sum_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralmesh_verts = _list_xs[0]\n",
    "X = _list_xs[0]\n",
    "for x in _list_xs[1:]:\n",
    "    X = np.concatenate((X, x), axis = 0)\n",
    "    # print(X)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the mean vertex coordinates over the sequence in \"E001_Neutral_Eyes_Opens\"\n",
    "- We assume that the average vertex coordinate over the sequence in \"E001_Neutral_Eyes_Open\" defines the ID's neutral face mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_neutralmesh_vertices = np.mean(neutralmesh_verts, axis = 0)\n",
    "print(ave_neutralmesh_vertices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PCA, we need to centrailzed data at the neutral face which is defined like above not at the average expression\n",
    "- We need to subtract the matrix from the average vertex coordinates of the sequence of neutral face (\"E001_Neutral_Eyes_Opened\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_X = X-ave_neutralmesh_vertices[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the contents in matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(X)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face mask\n",
    "- Masking the vertices to take into account only vertices composing the front face as you see below in blue\n",
    "\n",
    "- To do it, we only perform over these blue vertices.\n",
    "![face_mask_region](../images/Facemask_side01.png)\n",
    "\n",
    "- Approach\n",
    "    - In the data matrix, we replace vertices coordinate with 0 over the vertices composing other region "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Blendshape import FaceMask\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "\n",
    "# set the name of pickel file to be loaded\n",
    "pickel_fname = \"FaceMask_30102023_09_40.pkl\"\n",
    "\n",
    "facemask = load_from_memory(path_to_memory = save_path, pickle_fname = pickel_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cent_X = cent_X * facemask.bit_mask[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the data matrix is masked\n",
    "# we chose the ID:3567 which is located on top of nose \n",
    "# as the center of facemesh which is obtained by the nearest neighboring search at the vertex\n",
    "\n",
    "print(sum(masked_cent_X[:,3567*3]) != 0)\n",
    "print(sum(masked_cent_X[:,3567*3+1]) != 0)\n",
    "print(sum(masked_cent_X[:,3567*3+2]) != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on tracked meshes (min(#samples, #vertices) selected expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# The dimensions of space where can be represented by linear combination of principal components\n",
    "D = masked_cent_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if D == -1:\n",
    "    pca = PCA() #D=min(feature dimensions, the number of samples)\n",
    "else:\n",
    "    pca = PCA(D) #D\n",
    "\n",
    "pca.fit(masked_cent_X)\n",
    "Gamma = pca.components_ #right-hand side matrix of eigenvalue decomposition\n",
    "Variance = pca.explained_variance_\n",
    "Stds = np.sqrt(Variance)\n",
    "Sigma = np.diag(pca.explained_variance_) #diagonal matrix which has eigenvectors in each diagonal entry\n",
    "MEAN =  ave_neutralmesh_vertices # neutral face mesh (vertex coodinates of neutral face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Gamma (right-handside): {Gamma.shape}\")\n",
    "# print(Gamma)\n",
    "print(f\"Shape of Sigma (diagonal matrix): {Sigma.shape}\")\n",
    "# print(Sigma)\n",
    "print(f\"Shape of MEAN (average expressions): {MEAN.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the reconstruction error (E_RMS)\n",
    "- Root mean squared error (divided by 1000 as convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cent_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity check\n",
    "# close to 1: Sparse, close to 0: Dense\n",
    "sparsity_level = np.mean(Gamma==0)\n",
    "print(sparsity_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of sparsity of the principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.arange(0,Gamma.shape[1], 1)\n",
    "y_data = Gamma.T\n",
    "plt.plot(x_data, y_data[:,:3])\n",
    "plt.xlabel(\"Vertices\")\n",
    "plt.ylabel(\"Displacement\")\n",
    "plt.title(\"Components in matrix V (3 components from first row)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize the flipped cumulative sum over obtained principal components\n",
    "- As the corresponding value (y-axis) becomes lager, the component has larger global effect\n",
    "\n",
    "$$std_j = 1-  \\frac{std_i}{\\sum_j^{100}std_{j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_std = Stds.sum()\n",
    "print(sum_std)\n",
    "df_std = pd.DataFrame((Stds/sum_std))\n",
    "cumsum_std=df_std.cumsum()\n",
    "negative_cumsum_std = 1-cumsum_std\n",
    "negative_cumsum_std[:100].plot()\n",
    "plt.ylabel(\"std_deviations\")\n",
    "plt.xlabel(\"principal components\")\n",
    "# plt.xticks(np.arange(0, len(Stds)-1, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new expressions using PCs and Stds\n",
    "- Sample a vector of coefficients from a normal distribution not to deviate from mean\n",
    "D: how many principal components will be employed to represents a normal distribution\n",
    "\n",
    "$$a_i \\sim \\mathcal{N}(0, \\sigma_i^2)$$\n",
    "$$i = \\{0, D_{pcs}\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select D\n",
    "D_Pcs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = []\n",
    "for i, std in enumerate(Stds[:D_Pcs]):\n",
    "    mu,sigma = 0, std # mean and standard deviation\n",
    "    _noise = np.random.normal(mu, sigma, 1)\n",
    "    coefficients.append(_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add linear combination of principal components, in which are weighted by coefficients to mean vertex position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFace_vertices = MEAN\n",
    "for coeff in coefficients:\n",
    "    _item = coeff*Gamma[i]\n",
    "    newFace_vertices = newFace_vertices + _item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newFace_vertices)\n",
    "print(newFace_vertices.shape)\n",
    "print(newFace_vertices.shape[0]/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a .obj based on obtained vertex positions for visualization\n",
    "- Since the tracked mesh are topologically equivalent, we can easily obtain .obj only by rewriting line for vertex position (starting with 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path configuration\n",
    "sample_path = \"../dataset/multiface/tracked_mesh/sample.obj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ourput pointclouds (.obj) based on obtained vertex coordinates\n",
    ">>check /dataset/multiface/tracked_mesh/result_point_clouds.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample obj file should be read first\n",
    "obj = OBJ(sample_path, swapyz=False)\n",
    "obj.write_PointClouds(save_path, newFace_vertices, mesh_name = \"result_point_clouds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ourput mesh (.obj) based on obtained vertex coordinates\n",
    ">>check /dataset/multiface/tracked_mesh/result_mesh.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample obj file should be read first\n",
    "OBJ.write_OBJfile(reference_obj_file=sample_path, save_path=save_path, vertices=newFace_vertices, name_Exp=\"all-5Pcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of each blendshape\n",
    "- Output each blendshape\n",
    "- Compare with average face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Blendshape import Blendshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain each blenshape (.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blendshapes = []\n",
    "\n",
    "if not os.path.exists(os.path.join(save_path, \"First4Pcs_test\")):\n",
    "    os.mkdir(os.path.join(save_path, \"First4Pcs_test\"))\n",
    "\n",
    "\n",
    "OBJ.write_OBJfile(reference_obj_file= sample_path, save_path = save_path, vertices=MEAN, name_Exp=\"averageExp\")\n",
    "\n",
    "for i in range(1, 5):\n",
    "    _blendshape = Blendshape(Verts_averageExp=MEAN, PCs=Gamma, Stds = Stds, D = i, save_path=os.path.join(save_path, \"First4Pcs_test\"), only_specific_pc=True, name_newExp=str(i))\n",
    "    _blendshape.sample_coefficients()\n",
    "    _blendshape.get_newExp()\n",
    "    fname = _blendshape.generate_newExp()\n",
    "    blendshapes.append(_blendshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dump PCs, Stds, MEAN in .pickel for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Blendshape import datastruct_blendshape\n",
    "\n",
    "print(type(MEAN))\n",
    "print(type(Gamma))\n",
    "print(type(Stds))\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# get current date and year\n",
    "now = datetime.now()\n",
    "\n",
    "date = now.strftime(\"%d\") + now.strftime(\"%m\") + now.strftime(\"%Y\")\n",
    "print(date)\n",
    "time = now.strftime(\"%H_%M\")\n",
    "print(\"time:\", time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_blendshape = datastruct_blendshape(ID = ID, List_exps = list_exps_name, MEAN = MEAN, PCs = Gamma, Stds = Stds)\n",
    "# print(our_blendshape)\n",
    "pickel_fname = 'blendshape_'+date+'_'+time+'.pkl'\n",
    "# dump datastruct_blendshape\n",
    "dump_pckl(data=our_blendshape, save_root=save_path, pickel_fname=pickel_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(our_blendshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load blendshape from memory (pickel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_memory(path_to_memory=save_path, pickle_fname=pickel_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dsrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
