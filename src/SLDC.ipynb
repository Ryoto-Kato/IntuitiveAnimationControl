{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.OBJ_helper import OBJ\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from utils.pickel_io import load_from_memory\n",
    "from utils.Blendshape import ZeroMeanDefMatrix\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import splu\n",
    "from utils.Geodesic_dist import compute_topological_laplacian\n",
    "from utils.vis_tools import VisPointsAttributes\n",
    "save_path = \"../dataset/multiface/tracked_mesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# get current date and year\n",
    "now = datetime.now()\n",
    "\n",
    "date = now.strftime(\"%d\") + now.strftime(\"%m\") + now.strftime(\"%Y\")\n",
    "print(date)\n",
    "time = now.strftime(\"%H_%M\")\n",
    "print(\"time:\", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load meshes in trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# set path to dataset\n",
    "path_to_dataset = os.path.join(os.getcwd(), '../dataset/multiface/tracked_mesh/')\n",
    "\n",
    "ID = 6795937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of expressions\n",
    "list_exps_name = []\n",
    "map_exps2id = {}\n",
    "counter = 0\n",
    "for i, name in enumerate(os.listdir(path_to_dataset)):\n",
    "    f = os.path.join(path_to_dataset, name)\n",
    "    if os.path.isdir(f) and name.startswith('E0'):\n",
    "        counter = counter + 1\n",
    "        list_exps_name.append(name)\n",
    "\n",
    "\n",
    "list_exps_name.sort()\n",
    "\n",
    "for i, exp_name in enumerate(list_exps_name):\n",
    "    print(f'{i}, {exp_name}')\n",
    "    map_exps2id.update({exp_name: i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dataset_handler import Filehandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of .obj file name in respective expression folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_handler = Filehandler(path_to_dataset=path_to_dataset)\n",
    "print(file_handler.get_path_to_dataset())\n",
    "file_handler.iter_dir()\n",
    "print(\"Expressions: Number of tracked mesh\")\n",
    "for key in file_handler.dict_objs.keys():\n",
    "    print(f'{list_exps_name[key]}: {len(file_handler.dict_objs[key])}')\n",
    "    print(file_handler.dict_objs[key]) \n",
    "# print(file_handler.dict_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load mesh using trimesh or original mesh loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_loader = \"trimesh\"\n",
    "# mesh_loader = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_expMeshes = {}\n",
    "dict_expVerts = {}\n",
    "\n",
    "# selecgt number of samples for a expression \n",
    "num_samples_perExp = 20\n",
    "\n",
    "for expID, key in enumerate(file_handler.dict_objs.keys()):\n",
    "    list_Meshes = []\n",
    "    list_Verts = []\n",
    "\n",
    "    # since there are many sequences for a expression, we assume that second half of tracked mesh in a sequence captured the specific expressions\n",
    "    # We only use second half of sequence for a expression.\n",
    "    # This is also for resonable memory usage as well. if you run over all, you will consume more than 30GB memory to store all of objects\n",
    "\n",
    "    # half_id = int(len(file_handler.dict_objs[key])/2)\n",
    "    # end_id = int(len(file_handler.dict_objs[key]))\n",
    "\n",
    "    for i, obj in enumerate(file_handler.dict_objs[key][0:num_samples_perExp]):\n",
    "        path_to_obj = os.path.join(file_handler.list_expPathFiles[expID], obj)\n",
    "        # print(path_to_obj)\n",
    "        if mesh_loader == \"trimesh\":\n",
    "            _mesh = trimesh.load(path_to_obj, force='mesh')\n",
    "        elif mesh_loader == \"original\":\n",
    "            _mesh = OBJ(path_to_obj, swapyz=False)\n",
    "\n",
    "        list_Meshes.append(_mesh)\n",
    "        list_Verts.append(_mesh.vertices)\n",
    "    dict_expMeshes.update({expID: list_Meshes})\n",
    "    dict_expVerts.update({expID: list_Verts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vertices = len(dict_expVerts[0][0])\n",
    "len_col = num_vertices * 3\n",
    "print(f\"Number of vertex: {num_vertices}\")\n",
    "print(f\"The length of column: {len_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all vertex lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list_xs = []\n",
    "num_sum_samples = 0\n",
    "for key in dict_expVerts.keys():\n",
    "    vertices = dict_expVerts[key]\n",
    "    _num_samples = len(vertices)\n",
    "    # print(_num_samples)\n",
    "    num_sum_samples = num_sum_samples + _num_samples\n",
    "    # shape = [F, N*3]\n",
    "    # _array = np.array(vertices).reshape((_num_samples, len_col))\n",
    "    # shape = [F, N, 3]\n",
    "    _array = np.array(vertices)\n",
    "    _list_xs.append(_array)\n",
    "\n",
    "neutralmesh_verts = _list_xs[0]\n",
    "X = _list_xs[0]\n",
    "for x in _list_xs[1:]:\n",
    "    X = np.concatenate((X, x), axis = 0)\n",
    "    # print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain neutral face mesh vertex list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_neutralmesh_vertices = np.mean(neutralmesh_verts, axis = 0)\n",
    "print(ave_neutralmesh_vertices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centralized vertex coordinate at the neutral face mesh vertex position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_X = X - ave_neutralmesh_vertices[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centX_std = np.std(cent_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face mask\n",
    "- Masking the vertices to take into account only vertices composing the front face\n",
    "- To do it, we only perform methodology over masked region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Blendshape import FaceMask\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "\n",
    "# set the name of pickel file to be loaded\n",
    "if mesh_loader == \"trimesh\":\n",
    "    pickel_fname = \"FaceMask_29112023_11_23_trimesh.pkl\"\n",
    "elif mesh_loader == \"original\":\n",
    "    pickel_fname = \"FaceMask_30102023_09_40.pkl\"\n",
    "\n",
    "facemask = load_from_memory(path_to_memory = save_path, pickle_fname = pickel_fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(facemask.bit_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_cent_X = cent_X * facemask.bit_mask[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(os.path.join(save_path, \"sample.obj\"), force='mesh')\n",
    "tris = np.asarray(mesh.faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the data matrix if you need to dump the matrix to save loading time\n",
    "deformation_data = ZeroMeanDefMatrix(masked_cent_x = masked_cent_X, mean = ave_neutralmesh_vertices, std = centX_std, tris = tris)\n",
    "dd_pickel_fname = 'deformation_data_matrix_and_mean'+ '_' +date+'_'+time+'_'+mesh_loader+'.pkl'\n",
    "dump_pckl(data = deformation_data, save_root= save_path, pickel_fname=dd_pickel_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse localized deformation components\n",
    "- input: deformation matrix (zero-mean) \n",
    "$$\\mathbf{X} \\text{ (shape = [\\#trackedMeshes, \\#Vertices])}$$\n",
    "\n",
    "- output: sparse localized deformation component (shape = [#components, #vertices])\n",
    "$$\\mathbf{C} \\text{ (shape = [\\#Components, \\#Vertices])}$$\n",
    "    This is from the matrix factorization inducing sparsity in matrix $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "from utils.OBJ_helper import OBJ\n",
    "from utils.Blendshape import FaceMask\n",
    "from utils.Blendshape import ZeroMeanDefMatrix\n",
    "from utils.Geodesic_dist import compute_topological_laplacian\n",
    "from utils.vis_tools import VisPointsAttributes\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "from utils.Geodesic_dist import GeodesicDistHeatMethod, GeodesicDistSimple, compute_support_map\n",
    "from utils.converter import vector2MatNx3\n",
    "from utils.common_utils import project_weight, proxy_l1l2\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import splu\n",
    "save_path = \"../dataset/multiface/tracked_mesh\"\n",
    "\n",
    "# use efficient implementation of sparse Cholesky factorization.\n",
    "from sksparse.cholmod import cholesky_AAt, cholesky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformation_data_pickel_fname = \"deformation_data_matrix_and_mean_29112023_11_17_trimesh.pkl\"\n",
    "# load the deformation data matrix\n",
    "deformation_data = load_from_memory(path_to_memory=save_path, pickle_fname=deformation_data_pickel_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_cent_X: after centralized at mean, masked by bit mask\n",
    "masked_cent_X = deformation_data.masked_cent_x\n",
    "\n",
    "# MEAN: original vertex list of neutral face mesh\n",
    "MEAN = deformation_data.mean\n",
    "\n",
    "# std: standard deviation of cent_X(before masked)\n",
    "std = deformation_data.std\n",
    "\n",
    "# tris: triangle list (index tuples for triangle mesh)\n",
    "tris = deformation_data.tris\n",
    "\n",
    "Nverts = int(MEAN.shape[0]/3)\n",
    "if deformation_data_pickel_fname.endswith(\"_trimesh.pkl\"):\n",
    "    mesh_loader = \"trimesh\"\n",
    "else:\n",
    "    mesh_loader = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape of data matrix: {masked_cent_X.shape}\")\n",
    "print(f\"shape of mean mesh vertex array: {MEAN.shape}\")\n",
    "print(f\"shape of triangle list: {tris.shape}\")\n",
    "print(f\"std of data matrix: {std}\")\n",
    "print(f\"Number of vertices: {Nverts}\")\n",
    "print(f\"Mesh loader: {mesh_loader}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize mean mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.converter import vector2MatNx3\n",
    "# _MEAN_MatNx3 = vector2MatNx3(MEAN, Nverts)\n",
    "# VisPointsAttributes(_MEAN_MatNx3, None, cmap = 'jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Region Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain triangle list\n",
    "- Since the tracked meshes are topologically equivalent, we can get triangle list in advance from a sample.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- option1:load obj file using original loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get neutral face vertices and neutral face triangle list\n",
    "# target_obj = OBJ(filename = os.path.join(save_path, \"sample.obj\"))\n",
    "\n",
    "# list_vertices = target_obj.vertices\n",
    "# list_triangles = target_obj.faces\n",
    "\n",
    "# verts = np.asarray(list_vertices)\n",
    "\n",
    "# CENTER = 3567\n",
    "\n",
    "# tris = []\n",
    "# for triangle in list_triangles:\n",
    "#     # 0: triangle index list\n",
    "#     # 1: normals\n",
    "#     # 2: texture coordinate\n",
    "#     # 3: material configuration\n",
    "#     tris.append(triangle[0])\n",
    "# tris = np.asarray(tris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- option2:load obj file using trimesh loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(os.path.join(save_path, \"sample.obj\"), force='mesh')\n",
    "list_vertices = mesh.vertices\n",
    "list_triangles = mesh.faces\n",
    "\n",
    "verts = np.asarray(list_vertices)\n",
    "tris = np.asarray(list_triangles)\n",
    "\n",
    "CENTER = 2658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verts: (N, 3) array (float)\n",
    "# tris: (m, 3) array (int): indices into the verts array\n",
    "print(tris.shape)\n",
    "print(verts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triangle list conversion\n",
    "- index should be start from 0 to #num_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tris.min() > 0:\n",
    "    for triangle in tris:\n",
    "        for i in range(3):\n",
    "            # print(triangle)\n",
    "            triangle[i] = triangle[i] - int(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat method\n",
    "gdd = GeodesicDistHeatMethod(verts, tris)\n",
    "phi_heat = gdd(CENTER) #the vertex on top of a nose\n",
    "# visualize support map\n",
    "# gdd.visualize_distance_func()\n",
    "\n",
    "# simple method\n",
    "# simple_gdd = GeodesicDistSimple(verts=verts, tris=tris)\n",
    "# phi_simple = simple_gdd(CENTER)\n",
    "# simple_gdd.visualize_distance_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate support map (coefficient assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nornalized distance function\n",
    "Nphi_heat = phi_heat / max(phi_heat)\n",
    "min_dist = 0.05\n",
    "max_dist = 0.35\n",
    "support_map = compute_support_map(Nphi_heat, min_dist, max_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize support map (source #2658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize support map\n",
    "VisPointsAttributes(verts, support_map, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre computation\n",
    "- normalized masked/centralized vertex position into [-0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preScaleFactor = 1/std\n",
    "# Nmasked_cent_X = masked_cent_X*preScaleFactor\n",
    "N_cent_X = cent_X * preScaleFactor\n",
    "# R = Nmasked_cent_X.copy()\n",
    "R = N_cent_X.copy()\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components\n",
    "Ncompos = 300\n",
    "\n",
    "# minimum/maximum geodesic distance for support region \n",
    "srMinDist = 0.1\n",
    "srMaxDist = 0.5\n",
    "\n",
    "# number of iterations to run\n",
    "num_iters_max = 100\n",
    "\n",
    "# sparsity parameter (coeffient lambda for weight of L1 regularization term)\n",
    "sparse_lambda = 2.\n",
    "\n",
    "# pernalty parameter for ADMM (for multiplier)\n",
    "# Choice of ρ can greatly influence practical convergence of ADMM\n",
    "# TOO large: not enough emphasis on minimizing a f+z\n",
    "# TOO small: not enought emphasis on feasibility (Ax+Bz = c) \n",
    "rho = 10.0\n",
    "\n",
    "# number of iteration of ADMM\n",
    "num_admm_iterations = 10\n",
    "\n",
    "# geodesic distance computation on the mean verts\n",
    "gdd = GeodesicDistHeatMethod(MEAN, tris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "- Use deflation algorithm\n",
    "- for `k` in `num_components`\n",
    "    - Initialize $\\mathbf{W,C} = \\mathbf{0}$ and $\\mathbf{R} = \\mathbf{X}$\n",
    "    - Find the vertex j with the highest residual in matrix $\\mathbf{R}$\n",
    "    $$ j = \\text{argmax}_{j} \\mathbf{X} - \\mathbf{WC}$$\n",
    "    - Find the component $C_k$ and corresponding weights $W_{:,k}$ at each step that explain maximal variance in the data via SVD/PCA\n",
    "    - Subtract each of them from the deformataion matrix $\\mathbf{X}$ to compute residual $\\mathbf{R}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "W = []\n",
    "\n",
    "for k in range(Ncompos):\n",
    "    # find the vertex explaining the most variance across the residual matrix R\n",
    "    # take a norm of residual at each vertex\n",
    "    magnitude = (R**2).sum(axis = 2) #shape [FxN]\n",
    "\n",
    "    # vertex id with the most variance (residual)\n",
    "    idx = np.argmax(magnitude.sum(axis = 0))\n",
    "\n",
    "    # Find linear component explaining the motion of this vertex\n",
    "    # R: shape = [F, 3]\n",
    "    _U, s, Vh = linalg.svd(R[:, idx, :].reshape(R.shape[0], -1).T, full_matrices=False)\n",
    "    \n",
    "    # reconstruct column of matrix W at K-th column using most variant direction\n",
    "    w_k = s[0] * Vh[0, :]\n",
    "\n",
    "    # invert weight according to their projection onto the constraint set\n",
    "    # This prevent problems from having negative weights\n",
    "    wk_proj = project_weight(w_k)\n",
    "    wk_proj_negative = project_weight(-1*w_k)\n",
    "\n",
    "    # W_k will be replaced by the larger variance direction (+ or -)\n",
    "    if(linalg.norm(wk_proj) > linalg.norm(wk_proj_negative)):\n",
    "        w_k = wk_proj\n",
    "    else:\n",
    "        w_k = wk_proj_negative\n",
    "\n",
    "    # flipped support region\n",
    "    phi = gdd(idx)\n",
    "    phi/=max(phi)\n",
    "    flippedSR = 1 - compute_support_map(phi, srMinDist, srMaxDist)\n",
    "\n",
    "    # Solve normal equation to get C_k\n",
    "    # R: shape = [F, N, 3]\n",
    "    # W_K: shape = [F, 1]\n",
    "    # c_k: shape = [N, 3]\n",
    "    # flippedSR: shape = [N, ]\n",
    "    # W_k*C_k = flippedSR*R\n",
    "    # C_k = (W_k^T*W_k)^{-1} W_k^T*flippedSR*R\n",
    "\n",
    "    c_k = (np.tensordot(w_k, R, (0, 0)) * flippedSR[:, None])/ np.inner(w_k, w_k)\n",
    "\n",
    "    C.append(c_k)\n",
    "    W.append(w_k)\n",
    "\n",
    "    # update residual\n",
    "    R = R - np.outer(w_k, c_k).reshape(R.shape)\n",
    "\n",
    "C = np.array(C) #shape = [K, N, 3]\n",
    "W = np.array(W).T #shape = [F, K]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C.shape)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for matrix W (coefficient matrix)\n",
    "- The optimization problem w.r.t matrix W is separable due to the additional constraint\n",
    "- The constraints act on the weight vector $\\mathbf{W_{:, k}}$ of each component separately.\n",
    "- Use the block-coordinate descent algorithm, which optimize each column successively.\n",
    "- Then project the updated each column of W by projecting them onto the desired W space\n",
    "$$W'_{:, k} = \\text{argmin}_{\\mathbf{W_{:, k}\\in \\mathcal{V}}} ||\\mathbf{X} - \\mathbf{WC}||_{F}^2 = \\frac{(\\mathbf{R} + W_{:, k} C_k)\\cdot C_k}{C_k^TC_k}$$\n",
    "$$W' = \\frac{W'}{\\text{max}(W')}$$\n",
    "$$\\text{where } \\mathbf{R} = \\mathbf{X} - \\mathbf{WC}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for matrix C (deformation matrix)\n",
    "- Where we fixed matrix $\\mathbf{W}$, we can optimize C using convex optimization\n",
    "- Use ADMM (Alternating direction method of multipliers)\n",
    "    - This can optimize matrix C with good robostness of method of multipliers (faster than dual decomposition)\n",
    "    - This supports decomposition (method of multipliers does not support decomposition due to the quadoratic penalty)\n",
    "- Lasso problem $\\mathbf{Z}$\n",
    "$$\\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z})$$\n",
    "$$\\text{s.t. } \\mathbf{C} - \\mathbf{Z} = 0$$\n",
    "- Augumented Lagragian (Lagragian of ADMM)\n",
    "$$\\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z}) + \\mathbf{Y}^T(\\mathbf{C}-\\mathbf{Z})+ (\\frac{\\rho}{2})||\\mathbf{C}-\\mathbf{Z}||_2^2$$\n",
    "$$= \\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z}) + (\\frac{\\rho}{2})||\\mathbf{C}-\\mathbf{Z} + \\mathbf{U}||_2^2$$\n",
    "$$\\text{where } \\mathbf{U} = (\\frac{1}{\\rho})\\mathbf{Y}$$\n",
    "\n",
    "- The ADMM algorithm initializes $\\mathbf{U}\\in \\real^{K \\times 3N}$ to zero and then iterates the following steps.\n",
    "- Dual ascent\n",
    "$$C^* = \\text{argmin}_C ||X-WC||_{F}^2 + \\frac{\\rho}{2}||\\mathbf{C}-\\mathbf{Z}+\\mathbf{U}||_{F}^2 = (W^TW + \\rho I)^{-1} (W^TX+\\rho(Z-U))$$\n",
    "$$Z^* = \\text{argmin}_Z (\\Omega(\\mathbf{Z}) + \\frac{\\rho}{2}||\\mathbf{C^*}-\\mathbf{Z}+\\mathbf{U}||_{F}^2) = proxy_{\\rho}(0, (1-\\frac{\\Lambda_{i,k}}{\\rho ||\\mathbf{C}^* + \\mathbf{U}||_2^2}))_{+}[\\mathbf{C}^* + \\mathbf{U}]$$\n",
    "- Dual update\n",
    "$$\\mathbf{U}^* = \\mathbf{U} + \\mathbf{C}^* - \\mathbf{Z}^*$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nmasked_cent_X.shape\n",
    "N_cent_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_sparsity = np.sum( * np.sqrt((C**2).sum(axis = 2)))\n",
    "# original_error = (Nmasked_cent_X**2).sum()\n",
    "original_error = (N_cent_X**2).sum()\n",
    "print(original_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global optimization\n",
    "# F, N, _ = Nmasked_cent_X.shape\n",
    "F, N, _ = N_cent_X.shape\n",
    "\n",
    "Lambda = np.empty((Ncompos, N)) # each row representing the scaler of l1 penalty depending on the locality\n",
    "U = np.zeros_like(C)\n",
    "print(U.shape)\n",
    "\n",
    "for i in range(num_iters_max):\n",
    "    # Update weights\n",
    "    # fix weight matrix, optimize C (each row respectively: c_k)\n",
    "    Rflat = R.reshape(F, N*3) #flattened residual, shape = [F, N*3]\n",
    "    for k in range(C.shape[0]): # for c_k (kth row)\n",
    "        c_k = C[k].ravel() #flatten into [1, N*3]\n",
    "        ck_norm = np.inner(c_k, c_k)\n",
    "        if ck_norm <= 1e-8: # if the component does not represent any deformation component\n",
    "            W[:, k] = 0\n",
    "            continue # to prevent dividing by 0\n",
    "        \n",
    "        #block coordinate descent update\n",
    "        # get updated W[:,k]'\n",
    "        Rflat += np.outer(W[:, k], c_k) \n",
    "        opt = np.dot(Rflat, c_k) / ck_norm \n",
    "\n",
    "        #project W onto the desired space from constraints\n",
    "        W[:, k] = project_weight(opt)\n",
    "        Rflat -= np.outer(W[:, k], c_k)\n",
    "\n",
    "    # precomputing lambda for each component k (Regularization term)\n",
    "    # spatially varying regularization strength (to encode locality)\n",
    "    for k in range(Ncompos):\n",
    "        ck = C[k] #not flatten\n",
    "        # find vertex with the biggest displacement in component and computer support map around it\n",
    "        # take displacement vector norm at each vertex and find index with maximum of norm\n",
    "        idx = (ck**2).sum(axis = 1).argmax()\n",
    "        phi = gdd(idx)\n",
    "        phi/=max(phi)\n",
    "        support_map = compute_support_map(phi, srMinDist, srMaxDist)\n",
    "\n",
    "        # update L1 regularization strength according to this support map\n",
    "        Lambda[k] = sparse_lambda * support_map\n",
    "    \n",
    "    # TODO\n",
    "    # Inf or NaN check in W and C\n",
    "\n",
    "    # update components\n",
    "    Z = C.copy() # this is dual variable\n",
    "\n",
    "    # optimize matrix C fixing W\n",
    "    # prefactor linear solve in ADMM\n",
    "    G = np.dot(W.T, W)\n",
    "    # G[np.isfinite(G) == False] = 0\n",
    "    # c = np.dot(W.T, Nmasked_cent_X.reshape(Nmasked_cent_X.shape[0], -1)) #Nmasked_cent_X.reshaped into [F, N*3]  \n",
    "    c = np.dot(W.T, N_cent_X.reshape(N_cent_X.shape[0], -1)) #Nmasked_cent_X.reshaped into [F, N*3]  \n",
    "    # compute inverse part\n",
    "    # scipy\n",
    "    solve_prefactored = linalg.cho_factor(G + rho * np.eye(G.shape[0]))\n",
    "\n",
    "    # sksparse.cholmod\n",
    "    # sparse_csc_c = sparse.csc_matrix(G + rho * np.eye(G.shape[0]))\n",
    "    # solve_prefactored = cholesky(sparse_csc_c)\n",
    "\n",
    "    # ADMM iterations\n",
    "    # TODO\n",
    "    #    - check cho_factor and cho_solve from scipy\n",
    "    #    - create function for proxy of update of l1/l2 reguralization term\n",
    "    # old_U = U.reshape(U.shape[0], -1)\n",
    "    for admm_it in range(num_admm_iterations):\n",
    "        # temp_U = U.reshape(U.shape[0], -1)\n",
    "        # for i in range(temp_U.shape[0]):\n",
    "        #     for j in range(temp_U.shape[1]):\n",
    "        #         if not np.isfinite(temp_U[i][j]):\n",
    "        #             if old_U[i][j] != 0.0:\n",
    "        #                 print(f\"{i}, {j}: {old_U[i][j]}\")\n",
    "        rhs = c + rho * (Z.reshape(c.shape) - U.reshape(c.shape))\n",
    "        # rhs[np.isfinite(rhs)==False] = 0\n",
    "        C = linalg.cho_solve(solve_prefactored, rhs).reshape(C.shape)\n",
    "        # sparse_csc_rhs = sparse.csc_matrix(c + rho * (Z.reshape(c.shape) - U.reshape(c.shape)))\n",
    "        # sparse_csc_lhs = solve_prefactored(sparse_csc_rhs)\n",
    "        # C = sparse_csc_lhs.toarray().reshape(C.shape)\n",
    "        Z = proxy_l1l2(Lambda, C+U, 1.0/rho)\n",
    "        # old_U= U.reshape(U.shape[0], -1)\n",
    "        U = U + C - Z\n",
    "\n",
    "    # set updated components to dual Z\n",
    "    C = Z\n",
    "\n",
    "    # evaluate objective function\n",
    "    # R = Nmasked_cent_X - np.tensordot(W, C, (1, 0)) # residual\n",
    "    R = N_cent_X - np.tensordot(W, C, (1, 0)) # residual\n",
    "    if (i == 0):\n",
    "        initial_sparsity = np.sum(Lambda * np.sqrt((C**2).sum(axis = 2))) # L1 reguralization term \n",
    "        initial_reconst_error = ((X.reshape(X.shape[0], -1) - np.dot(W, C.reshape(C.shape[0], -1)))**2).sum()\n",
    "\n",
    "    sparsity = np.sum(Lambda * np.sqrt((C**2).sum(axis = 2))) # L1 reguralization term \n",
    "    reconst_error = ((X.reshape(X.shape[0], -1) - np.dot(W, C.reshape(C.shape[0], -1)))**2).sum()\n",
    "    print(f\"Reconstruction error: {(reconst_error/initial_reconst_error)}\")\n",
    "    print(f\"Sparsity: {sparsity/initial_sparsity}\")\n",
    "    e = ((reconst_error/initial_reconst_error)) + sparsity/initial_sparsity\n",
    "\n",
    "    # convergence check\n",
    "    print(\"iteration %03d, E=%f\" % (i, e))\n",
    "\n",
    "\n",
    "# undo scaling\n",
    "C /= preScaleFactor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isfinite(C).all()\n",
    "# np.isfinite(W).all()\n",
    "# # solve_prefactored[0]\n",
    "# np.isfinite(solve_prefactored[0]).all()\n",
    "# U[np.isfinite(U)==False]\n",
    "# np.isfinite(G).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = C * facemask.bit_mask[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.shape)\n",
    "print(C.reshape(C.shape[0], -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(W).all())\n",
    "print(np.isfinite(C).all())\n",
    "print(np.isfinite(solve_prefactored[0]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(C.shape[0]):\n",
    "    print(C[i].ravel())\n",
    "C[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(C.reshape(C.shape[0], -1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soarsity check\n",
    "# close to 1: Sparse, close to 0: Dense\n",
    "sparsity_level = np.mean(C==0)\n",
    "print(sparsity_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export blenshape components\n",
    "```\n",
    "@dataclass\n",
    "class datastruct_blendshape:\n",
    "    ID: int\n",
    "    List_exps: list\n",
    "    MEAN: np.ndarray\n",
    "    PCs: np.ndarray\n",
    "    Stds: np.ndarray\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Blendshape import datastruct_blendshape\n",
    "# load expression list\n",
    "example_data = load_from_memory(path_to_memory=save_path, pickle_fname='blendshape_SparsePCA_07112023_16_55.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_coefficients = np.max(W, axis = 0)\n",
    "min_coefficients = np.min(W, axis = 0)\n",
    "print(max_coefficients.shape)\n",
    "print(max_coefficients)\n",
    "print(min_coefficients.shape)\n",
    "print(min_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[:, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_export_ID = date+time\n",
    "_export_List_exps = example_data.List_exps\n",
    "_export_MEAN = MEAN.reshape(-1)\n",
    "_export_PCs = C.reshape(Ncompos,-1)\n",
    "_export_Stds = max_coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_export_ID)\n",
    "print(_export_List_exps)\n",
    "print(_export_MEAN.shape)\n",
    "print(_export_PCs.shape)\n",
    "print(_export_Stds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLDC_blendshape = datastruct_blendshape(ID = _export_ID, List_exps=_export_List_exps, MEAN=_export_MEAN, PCs=_export_PCs, Stds=_export_Stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickel_fname_SLDC = 'SLDC_blendshape_'+date+'_'+time+'.pkl'\n",
    "dump_pckl(data=SLDC_blendshape, save_root=save_path, pickel_fname=pickel_fname_SLDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pickel_io import load_from_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date + time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = load_from_memory(path_to_memory='../dataset/multiface/tracked_mesh', pickle_fname=pickel_fname_SLDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample.PCs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
