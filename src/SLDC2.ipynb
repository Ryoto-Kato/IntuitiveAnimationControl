{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.OBJ_helper import OBJ\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import trame\n",
    "from utils.pickel_io import load_from_memory\n",
    "from utils.Blendshape import DeformationComponents\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import splu\n",
    "from utils.Geodesic_dist import compute_topological_laplacian\n",
    "from utils.vis_tools import VisPointsAttributes\n",
    "path_to_sample = os.path.join(os.getcwd(), \"samples\", \"3dgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# get current date and year\n",
    "now = datetime.now()\n",
    "\n",
    "date = now.strftime(\"%d\") + now.strftime(\"%m\") + now.strftime(\"%Y\")\n",
    "print(date)\n",
    "time = now.strftime(\"%H_%M\")\n",
    "print(\"time:\", time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"trained_3dgs\"\n",
    "# dataset = \"tracked_mesh\"\n",
    "\n",
    "if dataset == \"tracked_mesh\":\n",
    "    path_to_dataset = os.path.join(os.getcwd(), \"samples\", \"deformation_components\", dataset)\n",
    "elif dataset == \"trained_3dgs\":\n",
    "    path_to_dataset = os.path.join(os.getcwd(), \"samples\", \"deformation_components\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse localized deformation components\n",
    "- input: deformation matrix (zero-mean) \n",
    "$$\\mathbf{X} \\text{ (shape = [\\#trackedMeshes, \\#Vertices])}$$\n",
    "\n",
    "- output: sparse localized deformation component (shape = [#components, #vertices])\n",
    "$$\\mathbf{C} \\text{ (shape = [\\#Components, \\#Vertices])}$$\n",
    "    This is from the matrix factorization inducing sparsity in matrix $C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "from utils.OBJ_helper import OBJ\n",
    "from utils.Blendshape import FaceMask\n",
    "from utils.Blendshape import ZeroMeanDefMatrix\n",
    "from utils.Geodesic_dist import compute_topological_laplacian\n",
    "from utils.vis_tools import VisPointsAttributes\n",
    "from utils.pickel_io import dump_pckl, load_from_memory\n",
    "from utils.Geodesic_dist import GeodesicDistHeatMethod, GeodesicDistSimple, compute_support_map, compute_support_map_gauss\n",
    "from utils.converter import vector2MatNx3\n",
    "from utils.common_utils import project_weight, proxy_l1l2\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "# use efficient implementation of sparse Cholesky factorization.\n",
    "from sksparse.cholmod import cholesky_AAt, cholesky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get identical data matrix from PCA and MBSPCA\n",
    "\n",
    "# tracked mesh\n",
    "if dataset == \"tracked_mesh\":\n",
    "    pca_hdf5 = \"tracked_mesh_5perExp_trimeshPCA_dcs.hdf5\"\n",
    "    pca_f = h5py.File(os.path.join(path_to_dataset, pca_hdf5), 'r')\n",
    "elif dataset == \"trained_3dgs\":\n",
    "    pca_hdf5 = \"Final_3dgs_87652_xyz_PCAMBSPCA_5perExp_trimesh_dcs.hdf5\"\n",
    "    pca_f = h5py.File(os.path.join(path_to_dataset, pca_hdf5), 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMat = np.asarray(pca_f['dataMat'])\n",
    "dataMat = dataMat.reshape((dataMat.shape[0], -1, 13))\n",
    "print(f\"Data matrix: {dataMat.shape}\")\n",
    "facemask = np.asarray(pca_f[\"faceMask\"])\n",
    "facemask = facemask.reshape((-1, 3))\n",
    "print(f\"Face mask: {facemask.shape}\")\n",
    "\n",
    "# MEAN: original vertex list of neutral face mesh\n",
    "# get the first frame of neutral face expression in the trained 3dgs\n",
    "MEAN = dataMat[0]\n",
    "print(f\"MEAN: {MEAN.shape}\")\n",
    "\n",
    "Gaussian_MEAN = np.asarray(pca_f['MEAN'])\n",
    "print(f\"Gaussian_MEAN: {Gaussian_MEAN.shape}\")\n",
    "\n",
    "Gaussian_STD = np.asarray(pca_f[\"STD\"])\n",
    "print(f\"Gaussian STD: {Gaussian_STD.shape}\")\n",
    "\n",
    "# tris: triangle list (index tuples for triangle mesh)\n",
    "tris = np.asarray(pca_f[\"tris\"])\n",
    "print(f\"triangle list: {tris.shape}\")\n",
    "\n",
    "# get the number of verts\n",
    "Nverts = int(MEAN.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a masked centralizedd matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centralized data matrix\n",
    "N_cent_X = dataMat\n",
    "print(N_cent_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select mesh loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_loader = \"trimesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape of data matrix: {N_cent_X.shape}\")\n",
    "print(f\"shape of triangle list: {tris.shape}\")\n",
    "print(f\"Number of vertices: {Nverts}\")\n",
    "print(f\"Mesh loader: {mesh_loader}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize mean mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.converter import vector2MatNx3\n",
    "# _MEAN_MatNx3 = vector2MatNx3(MEAN.flatten(), Nverts)\n",
    "# VisPointsAttributes(_MEAN_MatNx3, None, cmap = 'jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Region Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain triangle list\n",
    "- Since the tracked meshes are topologically equivalent, we can get triangle list in advance from a sample.obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load obj file using trimesh loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference verts and tris from the sample object file\n",
    "mesh = trimesh.load(os.path.join(path_to_sample, \"sample_face.ply\"), force='mesh')\n",
    "list_vertices = mesh.vertices\n",
    "list_triangles = mesh.faces\n",
    "verts = np.asarray(list_vertices)\n",
    "tris = np.asarray(list_triangles)\n",
    "\n",
    "# reference verts and tris from the first frame of the neutral expression\n",
    "# verts = dataMat[0]\n",
    "# tris = tris\n",
    "print(verts.shape)\n",
    "print(tris.shape)\n",
    "CENTER = 2658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_5509 = verts\n",
    "TRIS_5509 = tris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verts: (N, 3) array (float)\n",
    "# tris: (m, 3) array (int): indices into the verts array\n",
    "print(tris.shape)\n",
    "print(verts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triangle list conversion\n",
    "- index should be start from 0 to #num_vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tris.min() > 0:\n",
    "    for triangle in tris:\n",
    "        for i in range(3):\n",
    "            # print(triangle)\n",
    "            triangle[i] = triangle[i] - int(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat method\n",
    "gdd = GeodesicDistHeatMethod(verts, tris)\n",
    "phi = gdd(CENTER) #the vertex on top of a nose\n",
    "# visualize support map\n",
    "# gdd.visualize_distance_func()\n",
    "\n",
    "# simple method\n",
    "# simple_gdd = GeodesicDistSimple(verts=verts, tris=tris)\n",
    "# phi_simple = simple_gdd(CENTER)\n",
    "# simple_gdd.visualize_distance_func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_subd2(attribute, sample_mesh):\n",
    "    \"\"\"\n",
    "    sample_mesh: should be mesh in trimesh data structure (5509 verts)\n",
    "    attribute: attribute of each vertices (attribute should be in shape [5509, #attribs])\n",
    "    \"\"\"\n",
    "    subd1_vertices, subd1_faces = trimesh.remesh.subdivide(\n",
    "        vertices=np.hstack((sample_mesh.vertices, attribute)),\n",
    "        faces = sample_mesh.faces\n",
    "        )\n",
    "\n",
    "    subd1_verts = subd1_vertices[:, :3]\n",
    "    subd1_phiheat = subd1_vertices[:, 3:]\n",
    "\n",
    "    subd2_vertices, subd2_faces = trimesh.remesh.subdivide(\n",
    "        vertices=np.hstack((subd1_verts, subd1_phiheat)),\n",
    "        faces = subd1_faces\n",
    "    )\n",
    "\n",
    "    final_phi = subd2_vertices[:, 3:]\n",
    "    final_verts = subd2_vertices[:, :3]\n",
    "    return final_phi, final_verts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_heat, final_verts = attribute_subd2(attribute=phi.reshape(phi.shape[0], -1), sample_mesh=mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate support map (coefficient assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nornalized distance function\n",
    "Nphi_heat = phi_heat / max(phi_heat)\n",
    "min_dist = 0.01\n",
    "max_dist = 0.35\n",
    "support_map = compute_support_map(Nphi_heat, min_dist, max_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of sparsity assignment strategy (T-SLDC vs. Ours)\n",
    "- We use dummy distance data as sample to see each assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#Parameters to set\n",
    "mu_x = 0\n",
    "variance_x = 10\n",
    "\n",
    "mu_y = 0\n",
    "variance_y = 10\n",
    "\n",
    "#Create grid and multivariate normal\n",
    "x = np.linspace(-10,10,5000)\n",
    "y = np.linspace(-10,10,5000)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X; pos[:, :, 1] = Y\n",
    "rv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n",
    "\n",
    "dummy_dist = 1- (rv.pdf(pos)/np.max(rv.pdf(pos)))\n",
    "\n",
    "print(np.max(dummy_dist))\n",
    "# T-SLDC\n",
    "TSLDC_dummy_assignment = compute_support_map(dummy_dist, 0.1, 0.6)\n",
    "# Ours (local)\n",
    "Ours_dummy_assignment = compute_support_map_gauss(phi=dummy_dist, mu = 0, sigma=1.0)\n",
    "\n",
    "#Make a 3D plot for T-SLDC\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, TSLDC_dummy_assignment,cmap='jet',linewidth=0)\n",
    "plt.xticks(color=\"w\")  \n",
    "plt.yticks(color=\"w\")  \n",
    "ax.set_zlabel(r'$\\Lambda_{k}$')\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "# plt.savefig(\"Trapezoidal_SLDC\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Ours_dummy_assignment,cmap='jet',linewidth=0)\n",
    "plt.xticks(color=\"w\")\n",
    "plt.yticks(color=\"w\")\n",
    "ax.set_zlabel(r'$\\Lambda_{k}$')\n",
    "# plt.axis('off')\n",
    "plt.show()\n",
    "# plt.savefig(\"Ours_local\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize support map (source #2658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize support map\n",
    "VisPointsAttributes(final_verts, support_map, cmap = 'coolwarm', screenshot=True, title=\"supportregion_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre computation\n",
    "- normalized masked/centralized vertex position into [-0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for calculating reconstruction error w.r.t masked normalized centralized data matrix X\n",
    "R = N_cent_X[:, :5509, :].copy()\n",
    "dataMat_5509 = N_cent_X[:, :5509, :].copy()\n",
    "print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components\n",
    "Ncompos = 200\n",
    "\n",
    "# minimum/maximum geodesic distance for support region \n",
    "srMinDist = 0.2\n",
    "srMaxDist = 0.3\n",
    "\n",
    "# number of iterations to run\n",
    "num_iters_max = 20\n",
    "\n",
    "# sparsity parameter (coeffient lambda for weight of L1 regularization term)\n",
    "sparse_lambda = 2\n",
    "\n",
    "sample_5509_mesh = trimesh.load(os.path.join(path_to_sample, \"sample_face.ply\"), force='mesh')\n",
    "# pernalty parameter for ADMM (for multiplier)\n",
    "# Choice of Ï can greatly influence practical convergence of ADMM\n",
    "# TOO large: not enough emphasis on minimizing a f+z\n",
    "# TOO small: not enought emphasis on feasibility (Ax+Bz = c) \n",
    "rho = 10.0\n",
    "\n",
    "# number of iteration of ADMM\n",
    "num_admm_iterations = 10\n",
    "\n",
    "# geodesic distance computation on the mean verts\n",
    "gdd = GeodesicDistHeatMethod(MEAN_5509, TRIS_5509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "- Use deflation algorithm\n",
    "- for `k` in `num_components`\n",
    "    - Initialize $\\mathbf{W,C} = \\mathbf{0}$ and $\\mathbf{R} = \\mathbf{X}$\n",
    "    - Find the vertex j with the highest residual in matrix $\\mathbf{R}$\n",
    "    $$ j = \\text{argmax}_{j} \\mathbf{X} - \\mathbf{WC}$$\n",
    "    - Find the component $C_k$ and corresponding weights $W_{:,k}$ at each step that explain maximal variance in the data via SVD/PCA\n",
    "    - Subtract each of them from the deformataion matrix $\\mathbf{X}$ to compute residual $\\mathbf{R}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = []\n",
    "W = []\n",
    "\n",
    "for k in range(Ncompos):\n",
    "    # find the vertex explaining the most variance across the residual matrix R\n",
    "    # take a norm of residual at each vertex\n",
    "    magnitude = (R**2).sum(axis = 2) #shape [FxN]\n",
    "\n",
    "    # vertex id with the most variance (residual)\n",
    "    idx = np.argmax(magnitude.sum(axis = 0))\n",
    "\n",
    "    # Find linear component explaining the motion of this vertex\n",
    "    # R: shape = [F, 3]\n",
    "    _U, s, Vh = linalg.svd(R[:, idx, :].reshape(R.shape[0], -1).T, full_matrices=False)\n",
    "    \n",
    "    # reconstruct column of matrix W at K-th column using most variant direction\n",
    "    w_k = s[0] * Vh[0, :]\n",
    "\n",
    "    # invert weight according to their projection onto the constraint set\n",
    "    # This prevent problems from having negative weights\n",
    "    wk_proj = project_weight(w_k)\n",
    "    wk_proj_negative = project_weight(-1*w_k)\n",
    "\n",
    "    # W_k will be replaced by the larger variance direction (+ or -)\n",
    "    if(linalg.norm(wk_proj) > linalg.norm(wk_proj_negative)):\n",
    "        w_k = wk_proj\n",
    "    else:\n",
    "        w_k = wk_proj_negative\n",
    "\n",
    "    # flipped support region\n",
    "    phi = gdd(idx)\n",
    "    phi/=max(phi)\n",
    "    flippedSR = 1 - compute_support_map(phi, srMinDist, srMaxDist)\n",
    "\n",
    "    # Solve normal equation to get C_k\n",
    "    # R: shape = [F, N, 3]\n",
    "    # W_K: shape = [F, 1]\n",
    "    # c_k: shape = [N, 3]\n",
    "    # flippedSR: shape = [N, ]\n",
    "    # W_k*C_k = flippedSR*R\n",
    "    # C_k = (W_k^T*W_k)^{-1} W_k^T*flippedSR*R\n",
    "\n",
    "    c_k = (np.tensordot(w_k, R, (0, 0)) * flippedSR[:, None])/ np.inner(w_k, w_k)\n",
    "\n",
    "    C.append(c_k)\n",
    "    W.append(w_k)\n",
    "\n",
    "    # update residual\n",
    "    R = R - np.outer(w_k, c_k).reshape(R.shape)\n",
    "\n",
    "C = np.array(C) #shape = [K, N, 3]\n",
    "W = np.array(W).T #shape = [F, K]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C.shape)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for matrix W (coefficient matrix)\n",
    "- The optimization problem w.r.t matrix W is separable due to the additional constraint\n",
    "- The constraints act on the weight vector $\\mathbf{W_{:, k}}$ of each component separately.\n",
    "- Use the block-coordinate descent algorithm, which optimize each column successively.\n",
    "- Then project the updated each column of W by projecting them onto the desired W space\n",
    "$$W'_{:, k} = \\text{argmin}_{\\mathbf{W_{:, k}\\in \\mathcal{V}}} ||\\mathbf{X} - \\mathbf{WC}||_{F}^2 = \\frac{(\\mathbf{R} + W_{:, k} C_k)\\cdot C_k}{C_k^TC_k}$$\n",
    "$$W' = \\frac{W'}{\\text{max}(W')}$$\n",
    "$$\\text{where } \\mathbf{R} = \\mathbf{X} - \\mathbf{WC}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for matrix C (deformation matrix)\n",
    "- Where we fixed matrix $\\mathbf{W}$, we can optimize C using convex optimization\n",
    "- Use ADMM (Alternating direction method of multipliers)\n",
    "    - This can optimize matrix C with good robostness of method of multipliers (faster than dual decomposition)\n",
    "    - This supports decomposition (method of multipliers does not support decomposition due to the quadoratic penalty)\n",
    "- Lasso problem $\\mathbf{Z}$\n",
    "$$\\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z})$$\n",
    "$$\\text{s.t. } \\mathbf{C} - \\mathbf{Z} = 0$$\n",
    "- Augumented Lagragian (Lagragian of ADMM)\n",
    "$$\\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z}) + \\mathbf{Y}^T(\\mathbf{C}-\\mathbf{Z})+ (\\frac{\\rho}{2})||\\mathbf{C}-\\mathbf{Z}||_2^2$$\n",
    "$$= \\text{argmin}_{\\mathbf{C}, \\mathbf{Z}} ||\\mathbf{X} - \\mathbf{W} \\cdot \\mathbf{C}||_{F}^2 + \\Omega(\\mathbf{Z}) + (\\frac{\\rho}{2})||\\mathbf{C}-\\mathbf{Z} + \\mathbf{U}||_2^2$$\n",
    "$$\\text{where } \\mathbf{U} = (\\frac{1}{\\rho})\\mathbf{Y}$$\n",
    "\n",
    "- The ADMM algorithm initializes $\\mathbf{U}\\in \\real^{K \\times 3N}$ to zero and then iterates the following steps.\n",
    "- Dual ascent\n",
    "$$C^* = \\text{argmin}_C ||X-WC||_{F}^2 + \\frac{\\rho}{2}||\\mathbf{C}-\\mathbf{Z}+\\mathbf{U}||_{F}^2 = (W^TW + \\rho I)^{-1} (W^TX+\\rho(Z-U))$$\n",
    "$$Z^* = \\text{argmin}_Z (\\Omega(\\mathbf{Z}) + \\frac{\\rho}{2}||\\mathbf{C^*}-\\mathbf{Z}+\\mathbf{U}||_{F}^2) = proxy_{\\rho}(0, (1-\\frac{\\Lambda_{i,k}}{\\rho ||\\mathbf{C}^* + \\mathbf{U}||_2^2}))_{+}[\\mathbf{C}^* + \\mathbf{U}]$$\n",
    "- Dual update\n",
    "$$\\mathbf{U}^* = \\mathbf{U} + \\mathbf{C}^* - \\mathbf{Z}^*$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_sparsity = np.sum( * np.sqrt((C**2).sum(axis = 2)))\n",
    "# original_error = (Nmasked_cent_X**2).sum()\n",
    "original_error = (R**2).sum()\n",
    "print(original_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global optimization\n",
    "# F, N, _ = Nmasked_cent_X.shape\n",
    "F, N, _ = R.shape\n",
    "\n",
    "Lambda = np.empty((Ncompos, N)) # each row representing the scaler of l1 penalty depending on the locality\n",
    "U = np.zeros_like(C)\n",
    "print(U.shape)\n",
    "\n",
    "list_reconstruction_errors = []\n",
    "list_sparsity = []\n",
    "\n",
    "for i in range(num_iters_max):\n",
    "    # Update weights\n",
    "    # fix weight matrix, optimize C (each row respectively: c_k)\n",
    "    Rflat = R.reshape(F, N*3) #flattened residual, shape = [F, N*3]\n",
    "    for k in range(C.shape[0]): # for c_k (kth row)\n",
    "        c_k = C[k].ravel() #flatten into [1, N*3]\n",
    "        ck_norm = np.inner(c_k, c_k)\n",
    "        if ck_norm <= 1e-8: # if the component does not represent any deformation component\n",
    "            W[:, k] = 0\n",
    "            continue # to prevent dividing by 0\n",
    "        \n",
    "        #block coordinate descent update\n",
    "        # get updated W[:,k]'\n",
    "        Rflat += np.outer(W[:, k], c_k) \n",
    "        opt = np.dot(Rflat, c_k) / ck_norm \n",
    "\n",
    "        #project W onto the desired space from constraints\n",
    "        W[:, k] = project_weight(opt)\n",
    "        Rflat -= np.outer(W[:, k], c_k)\n",
    "\n",
    "    # precomputing lambda for each component k (Regularization term)\n",
    "    # spatially varying regularization strength (to encode locality)\n",
    "    for k in range(Ncompos):\n",
    "        ck = C[k] #not flatten\n",
    "        # find vertex with the biggest displacement in component and computer support map around it\n",
    "        # take displacement vector norm at each vertex and find index with maximum of norm\n",
    "        idx = (ck**2).sum(axis = 1).argmax()\n",
    "        phi = gdd(idx)\n",
    "        phi/=max(phi)\n",
    "        support_map = compute_support_map(phi, srMinDist, srMaxDist)\n",
    "\n",
    "        # update L1 regularization strength according to this support map\n",
    "        Lambda[k] = sparse_lambda * support_map\n",
    "    \n",
    "    # TODO\n",
    "    # Inf or NaN check in W and C\n",
    "\n",
    "    # update components\n",
    "    Z = C.copy() # this is dual variable\n",
    "\n",
    "    # optimize matrix C fixing W\n",
    "    # prefactor linear solve in ADMM\n",
    "    G = np.dot(W.T, W)\n",
    "    # G[np.isfinite(G) == False] = 0\n",
    "    # c = np.dot(W.T, Nmasked_cent_X.reshape(Nmasked_cent_X.shape[0], -1)) #Nmasked_cent_X.reshaped into [F, N*3]  \n",
    "    c = np.dot(W.T, R.reshape(R.shape[0], -1)) #Nmasked_cent_X.reshaped into [F, N*3]  \n",
    "    # compute inverse part\n",
    "    # scipy\n",
    "    solve_prefactored = linalg.cho_factor(G + rho * np.eye(G.shape[0]))\n",
    "\n",
    "    # sksparse.cholmod\n",
    "    # sparse_csc_c = sparse.csc_matrix(G + rho * np.eye(G.shape[0]))\n",
    "    # solve_prefactored = cholesky(sparse_csc_c)\n",
    "\n",
    "    # ADMM iterations\n",
    "    # TODO\n",
    "    #    - check cho_factor and cho_solve from scipy\n",
    "    #    - create function for proxy of update of l1/l2 reguralization term\n",
    "    # old_U = U.reshape(U.shape[0], -1)\n",
    "    for admm_it in range(num_admm_iterations):\n",
    "        # temp_U = U.reshape(U.shape[0], -1)\n",
    "        # for i in range(temp_U.shape[0]):\n",
    "        #     for j in range(temp_U.shape[1]):\n",
    "        #         if not np.isfinite(temp_U[i][j]):\n",
    "        #             if old_U[i][j] != 0.0:\n",
    "        #                 print(f\"{i}, {j}: {old_U[i][j]}\")\n",
    "        rhs = c + rho * (Z.reshape(c.shape) - U.reshape(c.shape))\n",
    "        # rhs[np.isfinite(rhs)==False] = 0\n",
    "        C = linalg.cho_solve(solve_prefactored, rhs).reshape(C.shape)\n",
    "        # sparse_csc_rhs = sparse.csc_matrix(c + rho * (Z.reshape(c.shape) - U.reshape(c.shape)))\n",
    "        # sparse_csc_lhs = solve_prefactored(sparse_csc_rhs)\n",
    "        # C = sparse_csc_lhs.toarray().reshape(C.shape)\n",
    "        Z = proxy_l1l2(Lambda, C+U, 1.0/rho)\n",
    "        # old_U= U.reshape(U.shape[0], -1)\n",
    "        U = U + C - Z\n",
    "\n",
    "    # set updated components to dual Z\n",
    "    C = Z\n",
    "\n",
    "    # evaluate objective function\n",
    "    # R = Nmasked_cent_X - np.tensordot(W, C, (1, 0)) # residual\n",
    "    R = R - np.tensordot(W, C, (1, 0)) # residual\n",
    "    if (i == 0):\n",
    "        initial_sparsity = np.sum(np.sqrt((C**2).sum(axis = 2))) # L1 reguralization term \n",
    "        initial_reconst_error = np.sqrt(((dataMat_5509.reshape(dataMat_5509.shape[0], -1) - np.dot(W, C.reshape(C.shape[0], -1)))**2).mean())\n",
    "\n",
    "    # sparsity = np.sum(np.sqrt(((C*facemask[None, :])**2).sum(axis = 2))) # L1/L2 reguralization term\n",
    "    sparsity = np.mean(C==0)\n",
    "    # reconstruction error: root mean squared error * 1000 for convenience\n",
    "    # reconst_error = np.sqrt(((X.reshape(X.shape[0], -1) - np.dot(W, C.reshape(C.shape[0], -1)))**2).mean())/1e3\n",
    "    reconst_error = np.sqrt(((dataMat_5509.reshape(dataMat_5509.shape[0], -1) - np.dot(W, C.reshape(C.shape[0], -1)))**2).mean())\n",
    "    # print(f\"Reconstruction error: {(reconst_error/initial_reconst_error)}\")\n",
    "    print(f\"Reconstruction error: {(reconst_error)}\")\n",
    "    list_reconstruction_errors.append(reconst_error)\n",
    "    list_sparsity.append(sparsity)\n",
    "    print(f\"Sparsity: {sparsity}\")\n",
    "    # e = ((reconst_error/initial_reconst_error)) + sparsity/initial_sparsity\n",
    "    e = ((reconst_error)) + sparsity\n",
    "\n",
    "    # convergence check\n",
    "    print(\"iteration %03d, E=%f\" % (i, e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isfinite(C).all()\n",
    "# np.isfinite(W).all()\n",
    "# # solve_prefactored[0]\n",
    "# np.isfinite(solve_prefactored[0]).all()\n",
    "# U[np.isfinite(U)==False]\n",
    "# np.isfinite(G).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(((dataMat_5509.reshape(R.shape[0], -1) - np.dot(W, (C).reshape(C.shape[0], -1)))**2).mean())\n",
    "print(f\"RMSE: {RMSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sparsity = np.sum(np.sqrt(((C)**2).sum(axis = 2)))\n",
    "print(Sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W.shape)\n",
    "print(C.reshape(C.shape[0], -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(W).all())\n",
    "print(np.isfinite(C).all())\n",
    "print(np.isfinite(solve_prefactored[0]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(C.shape[0]):\n",
    "#     # print(C[i].ravel())\n",
    "# C[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(C.reshape(C.shape[0], -1).T)\n",
    "plt.xlabel(\"Vertex index\")\n",
    "plt.ylabel(\"Deformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# error plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_reconstruction_errors)\n",
    "plt.xlabel(\"Iteration of global optimization\")\n",
    "plt.ylabel(\"RMSE (root mean squared error)\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparsity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_sparsity)\n",
    "plt.xlabel(\"Iteration of global optimization\")\n",
    "plt.ylabel(\"sparsity (L1/L2 norm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soarsity check\n",
    "# close to 1: Sparse, close to 0: Dense\n",
    "sparsity_level = np.mean(C==0)\n",
    "print(sparsity_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obtain the subdivided deformation components \n",
    "- Due to the size of covariance matrix of subd2_dataMat[87652, 87652], we need to sample deformation components by subdivision of the mesh with the deformation as their attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_C = C.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_C = temp_C[0]\n",
    "print(reshaped_C.shape)\n",
    "for i in range(1, temp_C.shape[0]):\n",
    "    reshaped_C = np.concatenate((reshaped_C, temp_C[i]), axis = 1)\n",
    "print(reshaped_C.shape)\n",
    "C_subd2, _ = attribute_subd2(attribute=reshaped_C, sample_mesh = sample_5509_mesh)\n",
    "print(C_subd2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_C = None\n",
    "final_C = C_subd2[:, :3].reshape(1, 87652, 3)\n",
    "for i in range (1, temp_C.shape[0]):\n",
    "    final_C = np.concatenate((final_C, C_subd2[:, 3*i:3*(i+1)].reshape(1, 87652, 3)), axis = 0)\n",
    "print(final_C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(final_C).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(((dataMat.reshape(dataMat.shape[0], -1) - np.dot(W, (final_C).reshape(C.shape[0], -1)))**2).mean())\n",
    "print(f\"RMSE: {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot of components (dcs) after 2x subdivision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_C.reshape(final_C.shape[0], -1).T)\n",
    "plt.xlabel(\"Vertex index\")\n",
    "plt.ylabel(\"Deformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export blenshape components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_saveName = \"3dgs_87652_xyz_SLDC_5perExp_trimesh_dcs.hdf5\"\n",
    "datasetName = \"3dgs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(os.path.join(path_to_dataset, hdf5_saveName), 'w')\n",
    "# verts 5509\n",
    "# [xyz, f_dc, rotation, scale]\n",
    "# [16527, 16527, 22036, 16527]\n",
    "# verts 87652\n",
    "# [xyz, f_dc, rotation, scale]\n",
    "# [262956, 262956, 350608, 262956]\n",
    "dc_key = datasetName\n",
    "list_attrName = [\"xyz\", \"f_dc\", \"rotation\", \"scale\"]\n",
    "# dim_info = [262956, 262956, 350608, 262956]\n",
    "# dset = f.create_dataset(name = \"dim_info\", data=np.asarray(dim_info))\n",
    "# dset.attrs[\"list_attrName\"] = str(list_attrName)\n",
    "dset1 = f.create_dataset(name = \"dataMat\", data=dataMat)\n",
    "dset2 = f.create_dataset(name = \"faceMask\", data=facemask)\n",
    "dset3 = f.create_dataset(name = \"MEAN\", data = Gaussian_MEAN)\n",
    "dset4 = f.create_dataset(name = \"STD\", data = Gaussian_STD)\n",
    "dset5 = f.create_dataset(name = \"sldc_W\", data = W)\n",
    "dset6 = f.create_dataset(name = \"sldc_C\", data = final_C)\n",
    "dset9 = f.create_dataset(name = \"tris\", data = tris)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if they are actually stored in hdf5\n",
    "# out_f = h5py.File(os.path.join(path_to_dataset, hdf5_saveName), 'r')\n",
    "# print(list(out_f.keys()))\n",
    "# out_set = out_f[\"dim_info\"]\n",
    "# print(out_f[\"sldc_W\"].shape)\n",
    "# print(out_f[\"sldc_C\"].shape)\n",
    "# print(out_set.attrs[\"list_attrName\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
